{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand Dataset: Multi-Domain Question Pairs\n",
    "\n",
    "**Goal:** Generate 150 question pairs across 3 domains to build a robust dataset for probing.\n",
    "\n",
    "**Domains:**\n",
    "- Geography: North/South relationships (50 pairs)\n",
    "- Dates: Historical event ordering (45 pairs)\n",
    "- Population: City/country size comparisons (45 pairs)\n",
    "\n",
    "**HITL Checkpoints:**\n",
    "- After each domain: Review contradiction rate and CoT quality\n",
    "- Before TransformerLens: Manual review of 10-15 contradiction examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: Setup - Clone repo and install package\n",
    "import os\n",
    "\n",
    "# Clone repo (only if not already cloned)\n",
    "if not os.path.exists('/content/MATS_Neel'):\n",
    "    !git clone https://github.com/YOUR_USERNAME/MATS_Neel.git\n",
    "    %cd /content/MATS_Neel\n",
    "else:\n",
    "    %cd /content/MATS_Neel\n",
    "    !git pull  # Get latest changes\n",
    "\n",
    "# Install dependencies\n",
    "!pip install torch transformers accelerate pandas -q\n",
    "\n",
    "# Install package in editable mode\n",
    "!pip install -e . -q\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Import from our package\n",
    "from src.data_generation import (\n",
    "    Domain,\n",
    "    LOCATION_PAIRS,\n",
    "    DATE_PAIRS,\n",
    "    POPULATION_PAIRS,\n",
    "    generate_geography_pairs,\n",
    "    generate_date_pairs,\n",
    "    generate_population_pairs,\n",
    "    SYSTEM_PROMPTS,\n",
    ")\n",
    "from src.labeling import extract_yes_no, detect_contradiction\n",
    "from src.experiment_utils import (\n",
    "    ExperimentConfig,\n",
    "    ExperimentResults,\n",
    "    create_experiment_run,\n",
    "    save_results,\n",
    "    log_domain_metrics,\n",
    "    finalize_results,\n",
    ")\n",
    "\n",
    "# Check GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: HuggingFace Authentication\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = None\n",
    "\n",
    "# Method 1: Colab Secrets\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    print(\"Found HF_TOKEN in Colab Secrets\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Method 2: Environment variable\n",
    "if not hf_token and \"HF_TOKEN\" in os.environ:\n",
    "    hf_token = os.environ[\"HF_TOKEN\"]\n",
    "    print(\"Found HF_TOKEN in environment\")\n",
    "\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"Logged in to HuggingFace\")\n",
    "else:\n",
    "    raise ValueError(\"No HF_TOKEN found. Add to Colab Secrets or environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Create experiment run\n",
    "config = ExperimentConfig(\n",
    "    name=\"expand_dataset_v1\",\n",
    "    description=\"Initial expanded dataset across 3 domains\",\n",
    "    domains=[\"geography\", \"dates\", \"population\"],\n",
    "    max_pairs_per_domain=50,\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.0,\n",
    "    notes=\"First run with multi-domain data\",\n",
    ")\n",
    "\n",
    "run_dir = create_experiment_run(\"expand_dataset\", config)\n",
    "print(f\"Experiment folder: {run_dir}\")\n",
    "\n",
    "# Initialize results\n",
    "results = ExperimentResults(\n",
    "    start_time=datetime.now().isoformat(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load model\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(f\"Model loaded on {device}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Generation function\n",
    "def generate_response(\n",
    "    question: str,\n",
    "    system_prompt: str,\n",
    "    max_new_tokens: int = 300,\n",
    ") -> str:\n",
    "    \"\"\"Generate a response from the model.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][input_ids.shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "def run_domain(\n",
    "    domain: Domain,\n",
    "    question_pairs: list[dict],\n",
    "    max_pairs: int = 50,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Run question pairs for a domain and collect results.\"\"\"\n",
    "    system_prompt = SYSTEM_PROMPTS[domain]\n",
    "    results_list = []\n",
    "    \n",
    "    pairs_to_run = question_pairs[:max_pairs]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running {domain.value.upper()}: {len(pairs_to_run)} pairs\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for i, pair in enumerate(pairs_to_run):\n",
    "        print(f\"\\nPair {i+1}/{len(pairs_to_run)}: {pair['entity_x']} vs {pair['entity_y']}\")\n",
    "        \n",
    "        # Generate responses\n",
    "        cot_a = generate_response(pair[\"question_a\"], system_prompt)\n",
    "        cot_b = generate_response(pair[\"question_b\"], system_prompt)\n",
    "        \n",
    "        # Extract answers\n",
    "        ans_a = extract_yes_no(cot_a)\n",
    "        ans_b = extract_yes_no(cot_b)\n",
    "        \n",
    "        # Check for contradiction\n",
    "        is_contradiction = detect_contradiction(ans_a, ans_b)\n",
    "        \n",
    "        if is_contradiction:\n",
    "            print(f\"  >>> CONTRADICTION: Both {ans_a}\")\n",
    "        else:\n",
    "            print(f\"  Answers: A={ans_a}, B={ans_b}\")\n",
    "        \n",
    "        results_list.append({\n",
    "            \"pair_id\": pair[\"pair_id\"],\n",
    "            \"domain\": domain.value,\n",
    "            \"entity_x\": pair[\"entity_x\"],\n",
    "            \"entity_y\": pair[\"entity_y\"],\n",
    "            \"difficulty\": pair[\"difficulty\"],\n",
    "            \"question_a\": pair[\"question_a\"],\n",
    "            \"question_b\": pair[\"question_b\"],\n",
    "            \"answer_a\": ans_a,\n",
    "            \"answer_b\": ans_b,\n",
    "            \"ground_truth_a\": pair[\"ground_truth_a\"],\n",
    "            \"ground_truth_b\": pair[\"ground_truth_b\"],\n",
    "            \"is_contradiction\": is_contradiction,\n",
    "            \"cot_a\": cot_a,\n",
    "            \"cot_b\": cot_b,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results_list)\n",
    "\n",
    "print(\"Functions ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Geography Domain\n",
    "\n",
    "**HITL Checkpoint:** After this section, review the contradiction rate.\n",
    "Target: >= 15% contradiction rate (we saw 40% in quick test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Run Geography\n",
    "geo_pairs = generate_geography_pairs()\n",
    "print(f\"Generated {len(geo_pairs)} geography pairs\")\n",
    "\n",
    "geo_df = run_domain(Domain.GEOGRAPHY, geo_pairs, max_pairs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Geography Summary\n",
    "geo_contradictions = geo_df[\"is_contradiction\"].sum()\n",
    "geo_rate = geo_contradictions / len(geo_df) * 100\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"GEOGRAPHY SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total pairs: {len(geo_df)}\")\n",
    "print(f\"Contradictions: {geo_contradictions} ({geo_rate:.1f}%)\")\n",
    "\n",
    "# Log metrics\n",
    "correct_a = (geo_df[\"answer_a\"] == geo_df[\"ground_truth_a\"]).sum()\n",
    "correct_b = (geo_df[\"answer_b\"] == geo_df[\"ground_truth_b\"]).sum()\n",
    "log_domain_metrics(results, \"geography\", len(geo_df), geo_contradictions, correct_a, correct_b)\n",
    "\n",
    "# Save intermediate results\n",
    "geo_df.to_csv(\"data/trajectories/geography.csv\", index=False)\n",
    "print(f\"\\nSaved to data/trajectories/geography.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Dates Domain\n",
    "\n",
    "**HITL Checkpoint:** Does the model show similar contradiction patterns with historical dates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Run Dates\n",
    "date_pairs = generate_date_pairs()\n",
    "print(f\"Generated {len(date_pairs)} date pairs\")\n",
    "\n",
    "date_df = run_domain(Domain.DATES, date_pairs, max_pairs=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Dates Summary\n",
    "date_contradictions = date_df[\"is_contradiction\"].sum()\n",
    "date_rate = date_contradictions / len(date_df) * 100\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATES SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total pairs: {len(date_df)}\")\n",
    "print(f\"Contradictions: {date_contradictions} ({date_rate:.1f}%)\")\n",
    "\n",
    "# Log metrics\n",
    "correct_a = (date_df[\"answer_a\"] == date_df[\"ground_truth_a\"]).sum()\n",
    "correct_b = (date_df[\"answer_b\"] == date_df[\"ground_truth_b\"]).sum()\n",
    "log_domain_metrics(results, \"dates\", len(date_df), date_contradictions, correct_a, correct_b)\n",
    "\n",
    "# Save\n",
    "date_df.to_csv(\"data/trajectories/dates.csv\", index=False)\n",
    "print(f\"\\nSaved to data/trajectories/dates.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Population Domain\n",
    "\n",
    "**HITL Checkpoint:** Population comparisons may have different error patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Run Population\n",
    "pop_pairs = generate_population_pairs()\n",
    "print(f\"Generated {len(pop_pairs)} population pairs\")\n",
    "\n",
    "pop_df = run_domain(Domain.POPULATION, pop_pairs, max_pairs=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Population Summary\n",
    "pop_contradictions = pop_df[\"is_contradiction\"].sum()\n",
    "pop_rate = pop_contradictions / len(pop_df) * 100\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"POPULATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total pairs: {len(pop_df)}\")\n",
    "print(f\"Contradictions: {pop_contradictions} ({pop_rate:.1f}%)\")\n",
    "\n",
    "# Log metrics\n",
    "correct_a = (pop_df[\"answer_a\"] == pop_df[\"ground_truth_a\"]).sum()\n",
    "correct_b = (pop_df[\"answer_b\"] == pop_df[\"ground_truth_b\"]).sum()\n",
    "log_domain_metrics(results, \"population\", len(pop_df), pop_contradictions, correct_a, correct_b)\n",
    "\n",
    "# Save\n",
    "pop_df.to_csv(\"data/trajectories/population.csv\", index=False)\n",
    "print(f\"\\nSaved to data/trajectories/population.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Overall Summary\n",
    "from datetime import datetime\n",
    "\n",
    "# Finalize results\n",
    "results.end_time = datetime.now().isoformat()\n",
    "finalize_results(results)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"OVERALL SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total pairs: {results.total_pairs}\")\n",
    "print(f\"Total contradictions: {results.total_contradictions}\")\n",
    "print(f\"Overall contradiction rate: {results.contradiction_rate:.1%}\")\n",
    "\n",
    "print(\"\\nPer-domain breakdown:\")\n",
    "for domain, metrics in results.domain_metrics.items():\n",
    "    print(f\"  {domain}: {metrics['contradictions']}/{metrics['total_pairs']} \"\n",
    "          f\"({metrics['contradiction_rate']:.1%})\")\n",
    "\n",
    "# Save final results\n",
    "save_results(run_dir, results)\n",
    "print(f\"\\nResults saved to {run_dir}/results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Combine all results\n",
    "all_df = pd.concat([geo_df, date_df, pop_df], ignore_index=True)\n",
    "all_df.to_csv(\"data/trajectories/all_domains.csv\", index=False)\n",
    "print(f\"Combined dataset saved: {len(all_df)} pairs\")\n",
    "\n",
    "# Quick stats\n",
    "print(\"\\nContradictions by difficulty:\")\n",
    "display(all_df.groupby(\"difficulty\")[\"is_contradiction\"].agg([\"sum\", \"count\", \"mean\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Display contradiction cases for review\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONTRADICTION CASES FOR MANUAL REVIEW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "contradiction_df = all_df[all_df[\"is_contradiction\"]]\n",
    "print(f\"\\nTotal contradiction cases: {len(contradiction_df)}\")\n",
    "\n",
    "# Show first 10 for manual review\n",
    "for i, (_, row) in enumerate(contradiction_df.head(10).iterrows()):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[{row['domain'].upper()}] {row['entity_x']} vs {row['entity_y']}\")\n",
    "    print(f\"Both answers: {row['answer_a']}\")\n",
    "    print(f\"\\n--- CoT A ---\")\n",
    "    print(row['cot_a'][:400] + \"...\" if len(row['cot_a']) > 400 else row['cot_a'])\n",
    "    print(f\"\\n--- CoT B ---\")\n",
    "    print(row['cot_b'][:400] + \"...\" if len(row['cot_b']) > 400 else row['cot_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Commit results to GitHub (optional)\n",
    "# Uncomment to save results back to repo\n",
    "\n",
    "# !git add data/trajectories/\n",
    "# !git add experiments/\n",
    "# !git commit -m \"Add expanded dataset results\"\n",
    "# !git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Gate\n",
    "\n",
    "Based on the results above:\n",
    "\n",
    "| Contradiction Rate | Action |\n",
    "|:-------------------|:-------|\n",
    "| >= 25% overall | Excellent! Proceed to TransformerLens activation extraction |\n",
    "| 15-25% overall | Good. May want to add more pairs, but can proceed |\n",
    "| < 15% overall | Review domain-specific rates. Focus on highest-yield domain |\n",
    "\n",
    "### Next Steps\n",
    "1. Download `data/trajectories/all_domains.csv` locally\n",
    "2. Manually review 10-15 contradiction cases\n",
    "3. Log observations in `experiments/<run_folder>/notes.md`\n",
    "4. If ready: proceed to `03_extract_activations.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
