{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Model Sycophancy Validation: DeepSeek\n",
    "\n",
    "**Goal:** Test if sycophancy direction from Llama-3-8B transfers to DeepSeek\n",
    "\n",
    "**Options:**\n",
    "1. TransformerLens with DeepSeek-R1-Distill-Llama-8B (Llama-based, may work)\n",
    "2. nnsight with NDIF (explicitly supports DeepSeek-R1)\n",
    "\n",
    "**Strategy:** Try TransformerLens first, fall back to nnsight if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: Setup\n",
    "import os\n",
    "\n",
    "# Clone repo if needed\n",
    "if not os.path.exists('/content/IPHR_Direction'):\n",
    "    !git clone https://github.com/nawidayima/IPHR_Direction.git\n",
    "    %cd /content/IPHR_Direction\n",
    "else:\n",
    "    %cd /content/IPHR_Direction\n",
    "    !git pull\n",
    "\n",
    "!pip install torch transformers accelerate pandas tqdm transformer_lens -q\n",
    "!pip install -e . -q\n",
    "\n",
    "print(\"Restart runtime if first run, then skip to Cell 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Try TransformerLens with DeepSeek-R1-Distill-Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Test TransformerLens compatibility\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "# DeepSeek-R1-Distill-Llama-8B uses Llama architecture\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "print(f\"Attempting to load {MODEL_NAME} with TransformerLens...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "try:\n",
    "    model = HookedTransformer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "    print(f\"SUCCESS! Model loaded with {model.cfg.n_layers} layers\")\n",
    "    USING_TRANSFORMERLENS = True\n",
    "except Exception as e:\n",
    "    print(f\"TransformerLens failed: {e}\")\n",
    "    print(\"Will try nnsight instead...\")\n",
    "    USING_TRANSFORMERLENS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Fallback to nnsight (if TransformerLens failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: nnsight fallback (only run if TransformerLens failed)\n",
    "if not USING_TRANSFORMERLENS:\n",
    "    !pip install nnsight -q\n",
    "    from nnsight import LanguageModel\n",
    "    \n",
    "    # Use NDIF remote service for DeepSeek\n",
    "    MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "    model = LanguageModel(MODEL_NAME)\n",
    "    print(f\"Loaded {MODEL_NAME} with nnsight\")\n",
    "    print(\"Will use remote=True for inference via NDIF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sycophancy Trajectories on DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load sycophancy utilities\n",
    "%cd /content/IPHR_Direction\n",
    "\n",
    "from src.sycophancy import (\n",
    "    QuestionCategory,\n",
    "    SycophancyLabel,\n",
    "    TrajectoryResult,\n",
    "    SYSTEM_PROMPT,\n",
    "    SCIENCE_QUESTIONS,\n",
    "    GEOGRAPHY_QUESTIONS,\n",
    "    STRONG_NEGATIVE_FEEDBACK_TEMPLATES,\n",
    "    extract_answer,\n",
    "    check_answer,\n",
    "    label_trajectory,\n",
    ")\n",
    "\n",
    "# Focus on science + geography (higher sycophancy rate)\n",
    "questions = SCIENCE_QUESTIONS + GEOGRAPHY_QUESTIONS\n",
    "print(f\"Using {len(questions)} questions (science + geography)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Generation function for TransformerLens\n",
    "if USING_TRANSFORMERLENS:\n",
    "    def generate_response(messages, max_new_tokens=100):\n",
    "        \"\"\"Generate response using TransformerLens model.\"\"\"\n",
    "        # Format as chat\n",
    "        prompt = \"\"\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                prompt += f\"<|system|>\\n{msg['content']}\\n\"\n",
    "            elif msg[\"role\"] == \"user\":\n",
    "                prompt += f\"<|user|>\\n{msg['content']}\\n\"\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                prompt += f\"<|assistant|>\\n{msg['content']}\\n\"\n",
    "        prompt += \"<|assistant|>\\n\"\n",
    "        \n",
    "        output = model.generate(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0,\n",
    "            stop_at_eos=True,\n",
    "        )\n",
    "        # Extract just the new tokens\n",
    "        response = output[len(prompt):].strip()\n",
    "        return response\n",
    "else:\n",
    "    # nnsight version\n",
    "    def generate_response(messages, max_new_tokens=100):\n",
    "        \"\"\"Generate response using nnsight with NDIF.\"\"\"\n",
    "        # Format prompt\n",
    "        prompt = \"\"\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                prompt += f\"System: {msg['content']}\\n\\n\"\n",
    "            elif msg[\"role\"] == \"user\":\n",
    "                prompt += f\"User: {msg['content']}\\n\\n\"\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                prompt += f\"Assistant: {msg['content']}\\n\\n\"\n",
    "        prompt += \"Assistant:\"\n",
    "        \n",
    "        with model.trace(prompt, remote=True):\n",
    "            output = model.output.save()\n",
    "        \n",
    "        # Decode output\n",
    "        response = model.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return response[len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Test generation\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "]\n",
    "test_response = generate_response(test_messages)\n",
    "print(f\"Test Q: What is the capital of France?\")\n",
    "print(f\"Test A: {test_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Generate trajectories\n",
    "random.seed(42)\n",
    "all_trajectories = []\n",
    "\n",
    "for q_idx, q in enumerate(tqdm(questions, desc=\"Generating\")):\n",
    "    question_id = f\"ds_q_{q_idx:03d}\"\n",
    "    feedback = random.choice(STRONG_NEGATIVE_FEEDBACK_TEMPLATES)\n",
    "    \n",
    "    try:\n",
    "        # Turn 1\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": q.question},\n",
    "        ]\n",
    "        first_response = generate_response(messages)\n",
    "        first_answer = extract_answer(first_response, q.category)\n",
    "        first_correct = check_answer(first_answer, q)\n",
    "        \n",
    "        # Turn 2 with strong negative feedback\n",
    "        messages.append({\"role\": \"assistant\", \"content\": first_response})\n",
    "        messages.append({\"role\": \"user\", \"content\": feedback})\n",
    "        second_response = generate_response(messages)\n",
    "        second_answer = extract_answer(second_response, q.category)\n",
    "        \n",
    "        answer_changed = first_answer and second_answer and (\n",
    "            first_answer.lower().strip() != second_answer.lower().strip()\n",
    "        )\n",
    "        label = label_trajectory(first_answer, second_answer, q, \"negative\")\n",
    "        \n",
    "        traj = TrajectoryResult(\n",
    "            question_id=question_id,\n",
    "            question=q.question,\n",
    "            correct_answer=q.correct_answer,\n",
    "            category=q.category.value,\n",
    "            first_response=first_response,\n",
    "            first_answer=first_answer,\n",
    "            first_correct=first_correct,\n",
    "            feedback_type=\"negative_strong\",\n",
    "            feedback=feedback,\n",
    "            second_response=second_response,\n",
    "            second_answer=second_answer,\n",
    "            answer_changed=answer_changed,\n",
    "            label=label,\n",
    "        )\n",
    "        all_trajectories.append(traj)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error at {question_id}: {e}\")\n",
    "\n",
    "print(f\"\\nGenerated {len(all_trajectories)} trajectories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Analyze results\n",
    "df = pd.DataFrame([t.to_dict() for t in all_trajectories])\n",
    "\n",
    "valid = df[df['first_correct'] == True]\n",
    "n_syc = (valid['label'] == 'sycophantic').sum()\n",
    "n_maintained = (valid['label'] == 'maintained').sum()\n",
    "n_total = len(valid)\n",
    "\n",
    "print(\"DeepSeek Sycophancy Results:\")\n",
    "print(f\"  Total trajectories: {len(df)}\")\n",
    "print(f\"  Valid (first correct): {n_total}\")\n",
    "print(f\"  Sycophantic: {n_syc} ({n_syc/n_total*100:.1f}%)\")\n",
    "print(f\"  Maintained: {n_maintained} ({n_maintained/n_total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save trajectories\n",
    "output_path = Path(\"experiments/deepseek_sycophancy.csv\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Activations for Cross-Model Probe Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Extract activations (TransformerLens version)\n",
    "if USING_TRANSFORMERLENS:\n",
    "    # Filter to valid negative feedback trajectories\n",
    "    valid_df = df[df['first_correct'] == True].reset_index(drop=True)\n",
    "    \n",
    "    # Layers to probe (similar to Llama experiment)\n",
    "    LAYERS = [12, 14, 16, 18, 20]\n",
    "    \n",
    "    activations = {layer: [] for layer in LAYERS}\n",
    "    labels = []\n",
    "    \n",
    "    print(f\"Extracting activations for {len(valid_df)} trajectories...\")\n",
    "    \n",
    "    for idx, row in tqdm(valid_df.iterrows(), total=len(valid_df)):\n",
    "        # Build prompt up to decision point (before second response)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": row['question']},\n",
    "            {\"role\": \"assistant\", \"content\": row['first_response']},\n",
    "            {\"role\": \"user\", \"content\": row['feedback']},\n",
    "        ]\n",
    "        \n",
    "        prompt = \"\"\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                prompt += f\"<|system|>\\n{msg['content']}\\n\"\n",
    "            elif msg[\"role\"] == \"user\":\n",
    "                prompt += f\"<|user|>\\n{msg['content']}\\n\"\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                prompt += f\"<|assistant|>\\n{msg['content']}\\n\"\n",
    "        prompt += \"<|assistant|>\\n\"\n",
    "        \n",
    "        # Run with cache\n",
    "        tokens = model.to_tokens(prompt)\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        # Get last token activation for each layer\n",
    "        for layer in LAYERS:\n",
    "            act = cache[f\"blocks.{layer}.hook_resid_post\"][0, -1, :].cpu().numpy()\n",
    "            activations[layer].append(act)\n",
    "        \n",
    "        # Label: 1 = sycophantic, 0 = maintained\n",
    "        labels.append(1 if row['label'] == 'sycophantic' else 0)\n",
    "        \n",
    "        # Clear cache\n",
    "        if idx % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    activations = {layer: np.array(acts) for layer, acts in activations.items()}\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    print(f\"Activations shape: {activations[16].shape}\")\n",
    "    print(f\"Labels: {labels.sum()} sycophantic, {len(labels) - labels.sum()} maintained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Model Probe Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Load Llama probe\n",
    "# Find the latest sycophancy experiment\n",
    "import glob\n",
    "\n",
    "probe_files = sorted(glob.glob(\"experiments/run_*_sycophancy/probes/sycophancy_probes.pt\"))\n",
    "if probe_files:\n",
    "    llama_probes = torch.load(probe_files[-1])\n",
    "    print(f\"Loaded Llama probes from: {probe_files[-1]}\")\n",
    "    print(f\"Layers: {list(llama_probes['dim_directions'].keys())}\")\n",
    "else:\n",
    "    print(\"ERROR: No Llama probes found. Run notebook 08 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Test Llama direction on DeepSeek activations\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"Cross-Model Transfer Test:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for layer in [16]:  # Start with best Llama layer\n",
    "    if layer not in llama_probes['dim_directions']:\n",
    "        print(f\"Layer {layer} not in Llama probes, skipping\")\n",
    "        continue\n",
    "    \n",
    "    llama_dir = llama_probes['dim_directions'][layer]\n",
    "    deepseek_acts = activations[layer]\n",
    "    \n",
    "    # Normalize direction\n",
    "    llama_dir_norm = llama_dir / np.linalg.norm(llama_dir)\n",
    "    \n",
    "    # Project DeepSeek activations onto Llama direction\n",
    "    projections = deepseek_acts @ llama_dir_norm\n",
    "    \n",
    "    # Compute AUC\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        auc = roc_auc_score(labels, projections)\n",
    "        print(f\"Layer {layer}: Llama direction on DeepSeek -> AUC = {auc:.3f}\")\n",
    "        \n",
    "        if auc > 0.7:\n",
    "            print(\"  -> TRANSFER SUCCESS! Same direction works across models.\")\n",
    "        elif auc > 0.6:\n",
    "            print(\"  -> Partial transfer, moderate signal.\")\n",
    "        else:\n",
    "            print(\"  -> No transfer, direction is model-specific.\")\n",
    "    else:\n",
    "        print(f\"Layer {layer}: Only one class present, cannot compute AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Train DeepSeek-specific probe and compare directions\n",
    "print(\"\\nDeepSeek-Specific Probe:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for layer in [16]:\n",
    "    X = activations[layer]\n",
    "    y = labels\n",
    "    \n",
    "    # DiM direction\n",
    "    syc_mask = y == 1\n",
    "    maintained_mask = y == 0\n",
    "    \n",
    "    if syc_mask.sum() < 2 or maintained_mask.sum() < 2:\n",
    "        print(f\"Layer {layer}: Not enough samples per class\")\n",
    "        continue\n",
    "    \n",
    "    deepseek_dir = X[syc_mask].mean(axis=0) - X[maintained_mask].mean(axis=0)\n",
    "    deepseek_dir_norm = deepseek_dir / np.linalg.norm(deepseek_dir)\n",
    "    \n",
    "    # Compute AUC with DeepSeek direction\n",
    "    projections = X @ deepseek_dir_norm\n",
    "    auc = roc_auc_score(y, projections)\n",
    "    print(f\"Layer {layer}: DeepSeek DiM direction -> AUC = {auc:.3f}\")\n",
    "    \n",
    "    # Compare to Llama direction\n",
    "    llama_dir = llama_probes['dim_directions'][layer]\n",
    "    llama_dir_norm = llama_dir / np.linalg.norm(llama_dir)\n",
    "    \n",
    "    cos_sim = np.dot(deepseek_dir_norm, llama_dir_norm)\n",
    "    print(f\"Layer {layer}: Cosine similarity (DeepSeekâ†”Llama) = {cos_sim:.3f}\")\n",
    "    \n",
    "    if abs(cos_sim) > 0.7:\n",
    "        print(\"  -> SHARED MECHANISM: Directions are highly similar!\")\n",
    "    elif abs(cos_sim) > 0.5:\n",
    "        print(\"  -> Partial overlap in directions.\")\n",
    "    else:\n",
    "        print(\"  -> Different mechanisms: Directions are orthogonal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Save DeepSeek activations and probes\n",
    "deepseek_data = {\n",
    "    'activations': activations,\n",
    "    'labels': labels,\n",
    "    'deepseek_dim_directions': {layer: activations[layer][labels==1].mean(0) - activations[layer][labels==0].mean(0) \n",
    "                                 for layer in LAYERS if labels.sum() > 0 and (1-labels).sum() > 0},\n",
    "    'metadata': valid_df.to_dict('records'),\n",
    "}\n",
    "\n",
    "torch.save(deepseek_data, \"experiments/deepseek_activations.pt\")\n",
    "print(\"Saved DeepSeek activations to experiments/deepseek_activations.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook tests cross-model transfer of the sycophancy direction.\n",
    "\n",
    "**Key results:**\n",
    "1. Llama direction AUC on DeepSeek: (computed above)\n",
    "2. DeepSeek-specific DiM AUC: (computed above)\n",
    "3. Cosine similarity between directions: (computed above)\n",
    "\n",
    "**Interpretation:**\n",
    "- If Llama direction AUC > 0.7 on DeepSeek -> direction transfers\n",
    "- If cosine similarity > 0.7 -> shared mechanism\n",
    "- Both would be a major finding for the paper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
