{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nawidayima/IPHR_Direction/blob/main/notebooks/pipeline_full_experiment.ipynb)\n",
    "\n",
    "# Full Sycophancy Experiment Pipeline\n",
    "\n",
    "**Goal:** Run the complete sycophancy experiment in a single notebook to minimize model loading overhead.\n",
    "\n",
    "**Optimizations:**\n",
    "- Single TransformerLens model for all Llama operations (gen + activation + steering)\n",
    "- Steering experiment runs BEFORE DeepSeek (only 2 model loads instead of 3)\n",
    "- Checkpoints after each section (resume if notebook crashes)\n",
    "- nnsight fallback for DeepSeek if TransformerLens fails\n",
    "\n",
    "**Estimated Runtime:** 3-4 hours on L4 GPU\n",
    "\n",
    "**Sections:**\n",
    "1. Setup & Config\n",
    "2. Load Llama (TransformerLens)\n",
    "3. Generate Llama Trajectories\n",
    "4. Extract Llama Activations\n",
    "5. Train Llama Probes\n",
    "6. Llama Steering Experiment\n",
    "7. DeepSeek Cross-Model Validation\n",
    "8. Visualizations\n",
    "9. Export & Download\n",
    "\n",
    "**Setup:** Add `HF_TOKEN` to Colab Secrets (key icon in sidebar), then Run All."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Setup & Config\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.1: Setup - Clone repo and install dependencies\n",
    "# NOTE: After running this cell, RESTART RUNTIME then run from Cell 1.2\n",
    "\n",
    "import os\n",
    "\n",
    "# Clone repo (only if not already cloned)\n",
    "if not os.path.exists('/content/IPHR_Direction'):\n",
    "    !git clone https://github.com/nawidayima/IPHR_Direction.git\n",
    "    %cd /content/IPHR_Direction\n",
    "else:\n",
    "    %cd /content/IPHR_Direction\n",
    "    !git pull\n",
    "\n",
    "# Install dependencies\n",
    "!pip install numpy==1.26.4 -q\n",
    "!pip install torch transformers accelerate pandas scipy tqdm matplotlib -q\n",
    "!pip install transformer_lens -q\n",
    "!pip install scikit-learn -q\n",
    "\n",
    "# Install package in editable mode\n",
    "!pip install -e . -q\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"IMPORTANT: Restart runtime now!\")\n",
    "print(\"Runtime > Restart runtime, then run from Cell 1.2\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1.2: Mount Google Drive (with retry logic)\nimport os\nimport time\n\nOUTPUT_DIR = None\n\ntry:\n    from google.colab import drive\n    \n    for attempt in range(3):\n        try:\n            drive.mount('/content/drive')\n            OUTPUT_DIR = '/content/drive/MyDrive/MATS_sycophancy'\n            os.makedirs(OUTPUT_DIR, exist_ok=True)\n            print(f\"Drive mounted! Output dir: {OUTPUT_DIR}\")\n            break\n        except Exception as e:\n            print(f\"Mount attempt {attempt+1} failed: {e}\")\n            time.sleep(5)\n    else:\n        print(\"WARNING: Drive mount failed, using local storage\")\n        OUTPUT_DIR = '/content/IPHR_Direction/experiments/pipeline_run'\n        os.makedirs(OUTPUT_DIR, exist_ok=True)\nexcept ImportError:\n    print(\"Not in Colab, using local storage\")\n    OUTPUT_DIR = 'experiments/pipeline_run'\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"Output directory: {OUTPUT_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.3: Imports and config\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import mcnemar\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Sklearn for probes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Will import TransformerLens after HF auth\n",
    "\n",
    "# Set seeds\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Config\n",
    "LLAMA_MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "DEEPSEEK_MODEL = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# Layer sweep (Arditi methodology)\n",
    "LAYERS = [4, 8, 12, 14, 16, 18, 20, 22, 24, 26, 28, 31]\n",
    "\n",
    "# Generation config\n",
    "MAX_NEW_TOKENS = 100\n",
    "TEMPERATURE = 0.0\n",
    "\n",
    "# Dataset expansion\n",
    "N_EXPANSION_RUNS = 3  # Additional runs with strong feedback\n",
    "\n",
    "# Paths\n",
    "OUTPUT_DIR = Path(OUTPUT_DIR)\n",
    "LLAMA_TRAJECTORIES_PATH = OUTPUT_DIR / \"llama_trajectories.csv\"\n",
    "LLAMA_ACTIVATIONS_PATH = OUTPUT_DIR / \"llama_activations.pt\"\n",
    "LLAMA_PROBES_PATH = OUTPUT_DIR / \"llama_probes.pt\"\n",
    "STEERING_RESULTS_PATH = OUTPUT_DIR / \"steering_results.json\"\n",
    "DEEPSEEK_TRAJECTORIES_PATH = OUTPUT_DIR / \"deepseek_trajectories.csv\"\n",
    "DEEPSEEK_ACTIVATIONS_PATH = OUTPUT_DIR / \"deepseek_activations.pt\"\n",
    "CROSS_MODEL_RESULTS_PATH = OUTPUT_DIR / \"cross_model_results.json\"\n",
    "\n",
    "# Check GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.4: HuggingFace Authentication\n",
    "hf_token = None\n",
    "\n",
    "# Method 1: Colab Secrets\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    print(\"Found HF_TOKEN in Colab Secrets\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Method 2: Environment variable\n",
    "if not hf_token and \"HF_TOKEN\" in os.environ:\n",
    "    hf_token = os.environ[\"HF_TOKEN\"]\n",
    "    print(\"Found HF_TOKEN in environment\")\n",
    "\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"Logged in to HuggingFace\")\n",
    "else:\n",
    "    raise ValueError(\"No HF_TOKEN found. Add to Colab Secrets or environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.5: Import sycophancy utilities\n",
    "%cd /content/IPHR_Direction\n",
    "\n",
    "from src.sycophancy import (\n",
    "    QuestionCategory,\n",
    "    FactualQuestion,\n",
    "    SycophancyLabel,\n",
    "    TrajectoryResult,\n",
    "    SYSTEM_PROMPT,\n",
    "    SCIENCE_QUESTIONS,\n",
    "    GEOGRAPHY_QUESTIONS,\n",
    "    get_all_questions,\n",
    "    get_feedback,\n",
    "    STRONG_NEGATIVE_FEEDBACK_TEMPLATES,\n",
    "    extract_answer,\n",
    "    check_answer,\n",
    "    label_trajectory,\n",
    ")\n",
    "\n",
    "# Focus on science + geography (higher sycophancy rate from prior experiments)\n",
    "questions = SCIENCE_QUESTIONS + GEOGRAPHY_QUESTIONS\n",
    "print(f\"Using {len(questions)} questions (science + geography)\")\n",
    "print(f\"\\nSample:\")\n",
    "for q in questions[:3]:\n",
    "    print(f\"  {q.question} -> {q.correct_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.6: Helper functions\n",
    "\n",
    "def checkpoint_exists(path):\n",
    "    \"\"\"Check if a checkpoint file exists.\"\"\"\n",
    "    return Path(path).exists()\n",
    "\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory and garbage collect.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"GPU memory cleared\")\n",
    "\n",
    "\n",
    "def format_multiturn_prompt(model, row: pd.Series) -> str:\n",
    "    \"\"\"Format conversation up to decision point (before second response).\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": row[\"question\"]},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"first_response\"]},\n",
    "        {\"role\": \"user\", \"content\": row[\"feedback\"]},\n",
    "    ]\n",
    "    return model.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Load Llama (TransformerLens)\n",
    "\n",
    "**Key optimization:** Using TransformerLens for BOTH generation AND activation extraction.\n",
    "This model instance will be used for Sections 2-6.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.1: Load Llama with TransformerLens\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "print(f\"Loading {LLAMA_MODEL} with TransformerLens...\")\n",
    "print(\"This may take 5-10 minutes...\")\n",
    "\n",
    "llama_model = HookedTransformer.from_pretrained(\n",
    "    LLAMA_MODEL,\n",
    "    fold_ln=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    device=\"cuda\",\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(f\"\\nModel loaded!\")\n",
    "print(f\"  Layers: {llama_model.cfg.n_layers}\")\n",
    "print(f\"  d_model: {llama_model.cfg.d_model}\")\n",
    "print(f\"  Heads: {llama_model.cfg.n_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.2: Test generation (TransformerLens can generate!)\n",
    "test_prompt = \"The capital of France is\"\n",
    "print(f\"Test prompt: {test_prompt}\")\n",
    "\n",
    "output = llama_model.generate(\n",
    "    test_prompt,\n",
    "    max_new_tokens=10,\n",
    "    temperature=0,\n",
    "    stop_at_eos=True,\n",
    ")\n",
    "print(f\"Model output: {output}\")\n",
    "\n",
    "# Test cache access\n",
    "tokens = llama_model.to_tokens(test_prompt)\n",
    "_, cache = llama_model.run_with_cache(tokens)\n",
    "print(f\"\\nCache access works! Shape at layer 16: {cache['resid_post', 16].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Generate Llama Trajectories\n",
    "\n",
    "**Goal:** Generate multi-turn sycophancy trajectories using strong negative feedback.\n",
    "Target: 30+ sycophantic examples for robust probe training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.1: Check for existing checkpoint\n",
    "if checkpoint_exists(LLAMA_TRAJECTORIES_PATH):\n",
    "    print(f\"Loading existing trajectories from {LLAMA_TRAJECTORIES_PATH}\")\n",
    "    llama_df = pd.read_csv(LLAMA_TRAJECTORIES_PATH)\n",
    "    print(f\"Loaded {len(llama_df)} trajectories\")\n",
    "    print(f\"Label distribution:\")\n",
    "    print(llama_df['label'].value_counts())\n",
    "    SKIP_GENERATION = True\n",
    "else:\n",
    "    print(\"No existing trajectories found. Will generate new ones.\")\n",
    "    SKIP_GENERATION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.2: Generation function using TransformerLens\n",
    "\n",
    "def generate_response_tl(model, messages, max_new_tokens=100):\n",
    "    \"\"\"Generate response using TransformerLens model.\"\"\"\n",
    "    prompt = model.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    \n",
    "    output = model.generate(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0,\n",
    "        stop_at_eos=True,\n",
    "    )\n",
    "    \n",
    "    # Remove prompt from output\n",
    "    response = output[len(prompt):].strip()\n",
    "    return response\n",
    "\n",
    "\n",
    "def generate_trajectory_tl(\n",
    "    model,\n",
    "    question: FactualQuestion,\n",
    "    feedback_type: str,\n",
    "    question_id: str,\n",
    ") -> TrajectoryResult:\n",
    "    \"\"\"Generate a full multi-turn trajectory using TransformerLens.\"\"\"\n",
    "    # Turn 1: Ask question\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": question.question},\n",
    "    ]\n",
    "    \n",
    "    first_response = generate_response_tl(model, messages)\n",
    "    first_answer = extract_answer(first_response, question.category)\n",
    "    first_correct = check_answer(first_answer, question)\n",
    "    \n",
    "    # Get feedback message (use strong negative for higher sycophancy rate)\n",
    "    if feedback_type == \"negative\":\n",
    "        feedback = random.choice(STRONG_NEGATIVE_FEEDBACK_TEMPLATES)\n",
    "    else:\n",
    "        feedback = get_feedback(feedback_type)\n",
    "    \n",
    "    # Turn 2: Give feedback, get second response\n",
    "    messages.append({\"role\": \"assistant\", \"content\": first_response})\n",
    "    messages.append({\"role\": \"user\", \"content\": feedback})\n",
    "    \n",
    "    second_response = generate_response_tl(model, messages)\n",
    "    second_answer = extract_answer(second_response, question.category)\n",
    "    \n",
    "    # Determine if answer changed\n",
    "    if first_answer and second_answer:\n",
    "        answer_changed = first_answer.lower().strip() != second_answer.lower().strip()\n",
    "    else:\n",
    "        answer_changed = False\n",
    "    \n",
    "    # Label the trajectory\n",
    "    label = label_trajectory(first_answer, second_answer, question, feedback_type)\n",
    "    \n",
    "    return TrajectoryResult(\n",
    "        question_id=question_id,\n",
    "        question=question.question,\n",
    "        correct_answer=question.correct_answer,\n",
    "        category=question.category.value,\n",
    "        first_response=first_response,\n",
    "        first_answer=first_answer,\n",
    "        first_correct=first_correct,\n",
    "        feedback_type=feedback_type,\n",
    "        feedback=feedback,\n",
    "        second_response=second_response,\n",
    "        second_answer=second_answer,\n",
    "        answer_changed=answer_changed,\n",
    "        label=label,\n",
    "    )\n",
    "\n",
    "\n",
    "# Quick test\n",
    "if not SKIP_GENERATION:\n",
    "    test_q = questions[0]\n",
    "    print(f\"Testing with: {test_q.question}\")\n",
    "    test_traj = generate_trajectory_tl(llama_model, test_q, \"negative\", \"test_001\")\n",
    "    print(f\"First answer: {test_traj.first_answer}\")\n",
    "    print(f\"Second answer: {test_traj.second_answer}\")\n",
    "    print(f\"Label: {test_traj.label.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.3: Generate trajectories (with strong negative feedback)\n",
    "\n",
    "if not SKIP_GENERATION:\n",
    "    print(f\"Generating trajectories for {len(questions)} questions...\")\n",
    "    print(f\"Using strong negative feedback for higher sycophancy rate\")\n",
    "    print(f\"Running {N_EXPANSION_RUNS} runs...\")\n",
    "    \n",
    "    all_trajectories = []\n",
    "    \n",
    "    for run_idx in range(N_EXPANSION_RUNS):\n",
    "        print(f\"\\n--- Run {run_idx + 1}/{N_EXPANSION_RUNS} ---\")\n",
    "        \n",
    "        for q_idx, q in enumerate(tqdm(questions, desc=f\"Run {run_idx + 1}\")):\n",
    "            question_id = f\"llama_r{run_idx}_q{q_idx:03d}\"\n",
    "            \n",
    "            try:\n",
    "                # Only negative feedback (focus on sycophancy detection)\n",
    "                traj = generate_trajectory_tl(llama_model, q, \"negative\", question_id)\n",
    "                all_trajectories.append(traj)\n",
    "            except Exception as e:\n",
    "                print(f\"Error at {question_id}: {e}\")\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if q_idx % 10 == 0:\n",
    "                clear_gpu_memory()\n",
    "        \n",
    "        # Check progress\n",
    "        temp_df = pd.DataFrame([t.to_dict() for t in all_trajectories])\n",
    "        valid = temp_df[(temp_df['first_correct'] == True) & (temp_df['label'].isin(['sycophantic', 'maintained']))]\n",
    "        n_syc = (valid['label'] == 'sycophantic').sum()\n",
    "        print(f\"Progress: {n_syc} sycophantic examples so far\")\n",
    "        \n",
    "        if n_syc >= 30:\n",
    "            print(f\"Target reached! Stopping early.\")\n",
    "            break\n",
    "    \n",
    "    # Create DataFrame\n",
    "    llama_df = pd.DataFrame([t.to_dict() for t in all_trajectories])\n",
    "    print(f\"\\nGenerated {len(llama_df)} trajectories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.4: Analyze and save trajectories\n",
    "\n",
    "# Filter to valid trajectories\n",
    "llama_valid = llama_df[\n",
    "    (llama_df['first_correct'] == True) & \n",
    "    (llama_df['label'].isin(['sycophantic', 'maintained']))\n",
    "].copy()\n",
    "\n",
    "n_syc = (llama_valid['label'] == 'sycophantic').sum()\n",
    "n_maintained = (llama_valid['label'] == 'maintained').sum()\n",
    "\n",
    "print(\"Llama Trajectory Summary:\")\n",
    "print(f\"  Total trajectories: {len(llama_df)}\")\n",
    "print(f\"  Valid for probing: {len(llama_valid)}\")\n",
    "print(f\"    Sycophantic: {n_syc}\")\n",
    "print(f\"    Maintained: {n_maintained}\")\n",
    "print(f\"  Sycophancy rate: {n_syc / len(llama_valid):.1%}\")\n",
    "\n",
    "if n_syc < 30:\n",
    "    print(f\"\\nWARNING: Only {n_syc} sycophantic examples (target: 30+)\")\n",
    "    print(\"Proceeding anyway, but probe may be underpowered.\")\n",
    "\n",
    "# Save checkpoint\n",
    "llama_df.to_csv(LLAMA_TRAJECTORIES_PATH, index=False)\n",
    "print(f\"\\nCHECKPOINT SAVED: {LLAMA_TRAJECTORIES_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Extract Llama Activations\n",
    "\n",
    "**Decision point:** First generated token after feedback (where model decides to maintain/change).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.1: Check for existing checkpoint\n",
    "if checkpoint_exists(LLAMA_ACTIVATIONS_PATH):\n",
    "    print(f\"Loading existing activations from {LLAMA_ACTIVATIONS_PATH}\")\n",
    "    llama_act_data = torch.load(LLAMA_ACTIVATIONS_PATH)\n",
    "    print(f\"Loaded activations for {llama_act_data['n_samples']} samples\")\n",
    "    SKIP_ACTIVATIONS = True\n",
    "else:\n",
    "    print(\"No existing activations found. Will extract new ones.\")\n",
    "    SKIP_ACTIVATIONS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.2: Extract activations\n",
    "\n",
    "if not SKIP_ACTIVATIONS:\n",
    "    print(f\"Extracting activations from {len(llama_valid)} valid trajectories...\")\n",
    "    print(f\"Layers: {LAYERS}\")\n",
    "    \n",
    "    activations = {layer: [] for layer in LAYERS}\n",
    "    labels = []\n",
    "    metadata = []\n",
    "    \n",
    "    for idx, (_, row) in enumerate(tqdm(llama_valid.iterrows(), total=len(llama_valid))):\n",
    "        try:\n",
    "            # Format prompt up to decision point\n",
    "            prompt = format_multiturn_prompt(llama_model, row)\n",
    "            tokens = llama_model.to_tokens(prompt)\n",
    "            \n",
    "            # Generate 1 token to capture decision state\n",
    "            with torch.no_grad():\n",
    "                logits = llama_model(tokens)\n",
    "                next_token = logits[0, -1, :].argmax().unsqueeze(0).unsqueeze(0)\n",
    "                tokens_extended = torch.cat([tokens, next_token], dim=1)\n",
    "                _, cache = llama_model.run_with_cache(tokens_extended)\n",
    "            \n",
    "            # Extract at first generated token\n",
    "            for layer in LAYERS:\n",
    "                act = cache[\"resid_post\", layer][0, -1, :].cpu().to(torch.float32)\n",
    "                activations[layer].append(act)\n",
    "            \n",
    "            # Label: 1 = sycophantic, 0 = maintained\n",
    "            labels.append(1 if row['label'] == 'sycophantic' else 0)\n",
    "            metadata.append({\n",
    "                'question_id': row['question_id'],\n",
    "                'category': row['category'],\n",
    "                'label': row['label'],\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error at idx {idx}: {e}\")\n",
    "        \n",
    "        if idx % 10 == 0:\n",
    "            clear_gpu_memory()\n",
    "    \n",
    "    # Stack into tensors\n",
    "    for layer in LAYERS:\n",
    "        activations[layer] = torch.stack(activations[layer])\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    print(f\"\\nExtracted {len(labels)} samples\")\n",
    "    print(f\"  Sycophantic: {labels.sum().item()}\")\n",
    "    print(f\"  Maintained: {(~labels.bool()).sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.3: Save activations checkpoint\n",
    "\n",
    "if not SKIP_ACTIVATIONS:\n",
    "    llama_act_data = {\n",
    "        'model_name': LLAMA_MODEL,\n",
    "        'layers': LAYERS,\n",
    "        'd_model': llama_model.cfg.d_model,\n",
    "        'activations': activations,\n",
    "        'labels': labels,\n",
    "        'metadata': metadata,\n",
    "        'n_samples': len(labels),\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "    }\n",
    "    \n",
    "    torch.save(llama_act_data, LLAMA_ACTIVATIONS_PATH)\n",
    "    print(f\"CHECKPOINT SAVED: {LLAMA_ACTIVATIONS_PATH}\")\n",
    "    print(f\"File size: {LLAMA_ACTIVATIONS_PATH.stat().st_size / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Train Llama Probes\n",
    "\n",
    "**Methods:**\n",
    "- Difference-in-Means (DiM): Simple direction from class means\n",
    "- Logistic Regression (LR): Learned linear classifier\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.1: Check for existing checkpoint\n",
    "if checkpoint_exists(LLAMA_PROBES_PATH):\n",
    "    print(f\"Loading existing probes from {LLAMA_PROBES_PATH}\")\n",
    "    llama_probe_data = torch.load(LLAMA_PROBES_PATH)\n",
    "    print(f\"Best DiM layer: {llama_probe_data['best_layer_dim']} (AUC={llama_probe_data['dim_aucs'][llama_probe_data['best_layer_dim']]:.4f})\")\n",
    "    SKIP_PROBES = True\n",
    "else:\n",
    "    print(\"No existing probes found. Will train new ones.\")\n",
    "    SKIP_PROBES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.2: Train probes\n",
    "\n",
    "if not SKIP_PROBES:\n",
    "    # Load activation data\n",
    "    activations = llama_act_data['activations']\n",
    "    labels = llama_act_data['labels']\n",
    "    metadata = llama_act_data['metadata']\n",
    "    \n",
    "    # Train/test split by question (prevent data leakage)\n",
    "    question_ids = [m['question_id'].split('_q')[1] for m in metadata]  # Group by question number\n",
    "    groups = np.array([int(qid[:3]) for qid in question_ids])  # Extract numeric part\n",
    "    \n",
    "    splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, test_idx = next(splitter.split(labels, labels, groups))\n",
    "    \n",
    "    print(f\"Train/test split: {len(train_idx)}/{len(test_idx)}\")\n",
    "    \n",
    "    # Store results\n",
    "    dim_directions = {}\n",
    "    dim_aucs = {}\n",
    "    lr_weights = {}\n",
    "    lr_biases = {}\n",
    "    lr_aucs = {}\n",
    "    \n",
    "    labels_np = labels.numpy()\n",
    "    \n",
    "    print(\"\\nTraining probes per layer...\")\n",
    "    for layer in tqdm(LAYERS, desc=\"Layers\"):\n",
    "        X = activations[layer].numpy()\n",
    "        \n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = labels_np[train_idx], labels_np[test_idx]\n",
    "        \n",
    "        # DiM: Difference in means\n",
    "        syc_mean = X_train[y_train == 1].mean(axis=0)\n",
    "        maint_mean = X_train[y_train == 0].mean(axis=0)\n",
    "        dim_dir = syc_mean - maint_mean\n",
    "        dim_dir_norm = dim_dir / np.linalg.norm(dim_dir)\n",
    "        \n",
    "        dim_projections = X_test @ dim_dir_norm\n",
    "        dim_auc = roc_auc_score(y_test, dim_projections) if len(np.unique(y_test)) > 1 else 0.5\n",
    "        \n",
    "        dim_directions[layer] = dim_dir_norm\n",
    "        dim_aucs[layer] = dim_auc\n",
    "        \n",
    "        # Logistic Regression\n",
    "        lr = LogisticRegression(C=1.0, max_iter=1000, random_state=42)\n",
    "        lr.fit(X_train, y_train)\n",
    "        lr_probs = lr.predict_proba(X_test)[:, 1]\n",
    "        lr_auc = roc_auc_score(y_test, lr_probs) if len(np.unique(y_test)) > 1 else 0.5\n",
    "        \n",
    "        lr_weights[layer] = lr.coef_[0]\n",
    "        lr_biases[layer] = lr.intercept_[0]\n",
    "        lr_aucs[layer] = lr_auc\n",
    "        \n",
    "        print(f\"Layer {layer:2d}: DiM AUC = {dim_auc:.4f}, LR AUC = {lr_auc:.4f}\")\n",
    "    \n",
    "    # Find best layers\n",
    "    best_layer_dim = max(dim_aucs, key=dim_aucs.get)\n",
    "    best_layer_lr = max(lr_aucs, key=lr_aucs.get)\n",
    "    \n",
    "    print(f\"\\nBest DiM: Layer {best_layer_dim} (AUC = {dim_aucs[best_layer_dim]:.4f})\")\n",
    "    print(f\"Best LR:  Layer {best_layer_lr} (AUC = {lr_aucs[best_layer_lr]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.3: Save probes checkpoint\n",
    "\n",
    "if not SKIP_PROBES:\n",
    "    llama_probe_data = {\n",
    "        'model_name': LLAMA_MODEL,\n",
    "        'layers': LAYERS,\n",
    "        'dim_directions': dim_directions,\n",
    "        'dim_aucs': dim_aucs,\n",
    "        'lr_weights': lr_weights,\n",
    "        'lr_biases': lr_biases,\n",
    "        'lr_aucs': lr_aucs,\n",
    "        'best_layer_dim': best_layer_dim,\n",
    "        'best_layer_lr': best_layer_lr,\n",
    "        'train_indices': train_idx,\n",
    "        'test_indices': test_idx,\n",
    "        'n_train': len(train_idx),\n",
    "        'n_test': len(test_idx),\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "    }\n",
    "    \n",
    "    torch.save(llama_probe_data, LLAMA_PROBES_PATH)\n",
    "    print(f\"CHECKPOINT SAVED: {LLAMA_PROBES_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Llama Steering Experiment\n",
    "\n",
    "**Intervention:** Directional ablation (Arditi et al.): `h' = h - (h . v̂) * v̂`\n",
    "\n",
    "**Key:** This runs BEFORE DeepSeek to reuse the same Llama model instance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.1: Check for existing checkpoint\n",
    "if checkpoint_exists(STEERING_RESULTS_PATH):\n",
    "    print(f\"Loading existing steering results from {STEERING_RESULTS_PATH}\")\n",
    "    with open(STEERING_RESULTS_PATH) as f:\n",
    "        steering_results = json.load(f)\n",
    "    print(f\"Baseline sycophancy: {steering_results['primary']['baseline_rate']:.1%}\")\n",
    "    print(f\"Ablated sycophancy: {steering_results['primary']['ablated_rate']:.1%}\")\n",
    "    SKIP_STEERING = True\n",
    "else:\n",
    "    print(\"No existing steering results found. Will run experiment.\")\n",
    "    SKIP_STEERING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.2: Define steering functions\n",
    "\n",
    "# Get best layer and direction\n",
    "STEER_LAYER = llama_probe_data['best_layer_dim']\n",
    "sycophancy_direction = llama_probe_data['dim_directions'][STEER_LAYER]\n",
    "sycophancy_dir_tensor = torch.tensor(sycophancy_direction, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "print(f\"Steering layer: {STEER_LAYER}\")\n",
    "print(f\"Probe AUC: {llama_probe_data['dim_aucs'][STEER_LAYER]:.4f}\")\n",
    "\n",
    "\n",
    "def ablate_sycophancy_hook(activation, hook, direction):\n",
    "    \"\"\"Remove sycophancy direction: h' = h - (h . v̂) * v̂\"\"\"\n",
    "    direction = direction.to(activation.device).to(activation.dtype)\n",
    "    projection = torch.einsum('bsd,d->bs', activation, direction)\n",
    "    return activation - torch.einsum('bs,d->bsd', projection, direction)\n",
    "\n",
    "\n",
    "def generate_baseline(model, row, max_new_tokens=100):\n",
    "    \"\"\"Generate without intervention.\"\"\"\n",
    "    prompt = format_multiturn_prompt(model, row)\n",
    "    output = model.generate(prompt, max_new_tokens=max_new_tokens, temperature=0, stop_at_eos=True)\n",
    "    return output[len(prompt):].strip()\n",
    "\n",
    "\n",
    "def generate_with_ablation(model, row, layer, direction, max_new_tokens=100):\n",
    "    \"\"\"Generate with sycophancy direction ablated.\"\"\"\n",
    "    prompt = format_multiturn_prompt(model, row)\n",
    "    hook_fn = lambda act, hook: ablate_sycophancy_hook(act, hook, direction)\n",
    "    hook_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "    \n",
    "    with model.hooks([(hook_name, hook_fn)]):\n",
    "        output = model.generate(prompt, max_new_tokens=max_new_tokens, temperature=0, stop_at_eos=True)\n",
    "    return output[len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.3: Run steering experiment on test set\n",
    "\n",
    "if not SKIP_STEERING:\n",
    "    # Get test indices\n",
    "    test_indices = llama_probe_data['test_indices']\n",
    "    df_test = llama_valid.iloc[test_indices].copy()\n",
    "    \n",
    "    print(f\"Running steering experiment on {len(df_test)} test samples...\")\n",
    "    \n",
    "    test_results = []\n",
    "    \n",
    "    for idx, (_, row) in enumerate(tqdm(df_test.iterrows(), total=len(df_test))):\n",
    "        try:\n",
    "            category = QuestionCategory(row['category'])\n",
    "            \n",
    "            # Generate baseline\n",
    "            baseline_response = generate_baseline(llama_model, row)\n",
    "            baseline_answer = extract_answer(baseline_response, category)\n",
    "            \n",
    "            # Generate with ablation\n",
    "            ablated_response = generate_with_ablation(\n",
    "                llama_model, row, STEER_LAYER, sycophancy_dir_tensor\n",
    "            )\n",
    "            ablated_answer = extract_answer(ablated_response, category)\n",
    "            \n",
    "            # Check if answers changed\n",
    "            first_answer = row['first_answer']\n",
    "            \n",
    "            baseline_changed = (\n",
    "                first_answer and baseline_answer and\n",
    "                first_answer.lower().strip() != baseline_answer.lower().strip()\n",
    "            )\n",
    "            ablated_changed = (\n",
    "                first_answer and ablated_answer and\n",
    "                first_answer.lower().strip() != ablated_answer.lower().strip()\n",
    "            )\n",
    "            \n",
    "            test_results.append({\n",
    "                'question_id': row['question_id'],\n",
    "                'first_answer': first_answer,\n",
    "                'baseline_answer': baseline_answer,\n",
    "                'baseline_changed': baseline_changed,\n",
    "                'ablated_answer': ablated_answer,\n",
    "                'ablated_changed': ablated_changed,\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error at idx {idx}: {e}\")\n",
    "        \n",
    "        if idx % 5 == 0:\n",
    "            clear_gpu_memory()\n",
    "    \n",
    "    print(f\"\\nCompleted {len(test_results)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.4: Analyze and save steering results\n",
    "\n",
    "if not SKIP_STEERING:\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "    valid = results_df.dropna(subset=['baseline_changed', 'ablated_changed'])\n",
    "    \n",
    "    n_valid = len(valid)\n",
    "    baseline_syc = valid['baseline_changed'].sum()\n",
    "    ablated_syc = valid['ablated_changed'].sum()\n",
    "    \n",
    "    baseline_rate = baseline_syc / n_valid\n",
    "    ablated_rate = ablated_syc / n_valid\n",
    "    reduction = (baseline_syc - ablated_syc) / baseline_syc if baseline_syc > 0 else 0\n",
    "    \n",
    "    # McNemar contingency table\n",
    "    a = ((valid['baseline_changed'] == True) & (valid['ablated_changed'] == True)).sum()\n",
    "    b = ((valid['baseline_changed'] == True) & (valid['ablated_changed'] == False)).sum()  # Helped\n",
    "    c = ((valid['baseline_changed'] == False) & (valid['ablated_changed'] == True)).sum()  # Hurt\n",
    "    d = ((valid['baseline_changed'] == False) & (valid['ablated_changed'] == False)).sum()\n",
    "    \n",
    "    print(\"STEERING RESULTS (Test Set Only)\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Valid samples: {n_valid}\")\n",
    "    print(f\"Baseline sycophancy: {baseline_syc}/{n_valid} = {baseline_rate:.1%}\")\n",
    "    print(f\"Ablated sycophancy:  {ablated_syc}/{n_valid} = {ablated_rate:.1%}\")\n",
    "    print(f\"Reduction: {reduction:.1%}\")\n",
    "    print(f\"\\nAblation helped: {b} cases\")\n",
    "    print(f\"Ablation hurt: {c} cases\")\n",
    "    \n",
    "    # Save results\n",
    "    steering_results = {\n",
    "        'model_name': LLAMA_MODEL,\n",
    "        'steer_layer': STEER_LAYER,\n",
    "        'probe_auc': llama_probe_data['dim_aucs'][STEER_LAYER],\n",
    "        'primary': {\n",
    "            'n_samples': int(n_valid),\n",
    "            'baseline_sycophancy': int(baseline_syc),\n",
    "            'ablated_sycophancy': int(ablated_syc),\n",
    "            'baseline_rate': baseline_rate,\n",
    "            'ablated_rate': ablated_rate,\n",
    "            'reduction': reduction,\n",
    "            'helped': int(b),\n",
    "            'hurt': int(c),\n",
    "        },\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "    }\n",
    "    \n",
    "    with open(STEERING_RESULTS_PATH, 'w') as f:\n",
    "        json.dump(steering_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nCHECKPOINT SAVED: {STEERING_RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.5: Unload Llama model\n",
    "print(\"Unloading Llama model...\")\n",
    "del llama_model\n",
    "clear_gpu_memory()\n",
    "print(\"Llama model unloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: DeepSeek Cross-Model Validation\n",
    "\n",
    "**Goal:** Test if the Llama sycophancy direction transfers to DeepSeek.\n",
    "\n",
    "**Fallback:** nnsight if TransformerLens fails.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.1: Try loading DeepSeek with TransformerLens\n",
    "print(f\"Attempting to load {DEEPSEEK_MODEL} with TransformerLens...\")\n",
    "\n",
    "USING_TRANSFORMERLENS = False\n",
    "\n",
    "try:\n",
    "    deepseek_model = HookedTransformer.from_pretrained(\n",
    "        DEEPSEEK_MODEL,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "    print(f\"SUCCESS! DeepSeek loaded with TransformerLens\")\n",
    "    print(f\"  Layers: {deepseek_model.cfg.n_layers}\")\n",
    "    USING_TRANSFORMERLENS = True\n",
    "except Exception as e:\n",
    "    print(f\"TransformerLens failed: {e}\")\n",
    "    print(\"Will try nnsight instead...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.2: Fallback to nnsight if needed\n",
    "if not USING_TRANSFORMERLENS:\n",
    "    print(\"Installing and loading nnsight...\")\n",
    "    !pip install nnsight -q\n",
    "    \n",
    "    from nnsight import LanguageModel\n",
    "    \n",
    "    deepseek_model = LanguageModel(DEEPSEEK_MODEL)\n",
    "    print(f\"Loaded {DEEPSEEK_MODEL} with nnsight\")\n",
    "    print(\"Note: Using remote=True for NDIF inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.3: Define DeepSeek generation functions\n",
    "\n",
    "if USING_TRANSFORMERLENS:\n",
    "    def generate_deepseek(messages, max_new_tokens=100):\n",
    "        \"\"\"Generate with DeepSeek using TransformerLens.\"\"\"\n",
    "        # DeepSeek uses different chat template\n",
    "        prompt = \"\"\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                prompt += f\"<|system|>\\n{msg['content']}\\n\"\n",
    "            elif msg[\"role\"] == \"user\":\n",
    "                prompt += f\"<|user|>\\n{msg['content']}\\n\"\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                prompt += f\"<|assistant|>\\n{msg['content']}\\n\"\n",
    "        prompt += \"<|assistant|>\\n\"\n",
    "        \n",
    "        output = deepseek_model.generate(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0,\n",
    "            stop_at_eos=True,\n",
    "        )\n",
    "        return output[len(prompt):].strip()\n",
    "else:\n",
    "    def generate_deepseek(messages, max_new_tokens=100):\n",
    "        \"\"\"Generate with DeepSeek using nnsight.\"\"\"\n",
    "        prompt = \"\"\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                prompt += f\"System: {msg['content']}\\n\\n\"\n",
    "            elif msg[\"role\"] == \"user\":\n",
    "                prompt += f\"User: {msg['content']}\\n\\n\"\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                prompt += f\"Assistant: {msg['content']}\\n\\n\"\n",
    "        prompt += \"Assistant:\"\n",
    "        \n",
    "        with deepseek_model.trace(prompt, remote=True):\n",
    "            output = deepseek_model.output.save()\n",
    "        \n",
    "        response = deepseek_model.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return response[len(prompt):].strip()\n",
    "\n",
    "\n",
    "# Test\n",
    "test_msg = [{\"role\": \"user\", \"content\": \"What is 2 + 2?\"}]\n",
    "test_out = generate_deepseek(test_msg)\n",
    "print(f\"Test: 2 + 2 = {test_out[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.4: Generate DeepSeek trajectories\n",
    "\n",
    "if checkpoint_exists(DEEPSEEK_TRAJECTORIES_PATH):\n",
    "    print(f\"Loading existing DeepSeek trajectories...\")\n",
    "    deepseek_df = pd.read_csv(DEEPSEEK_TRAJECTORIES_PATH)\n",
    "    SKIP_DEEPSEEK_GEN = True\n",
    "else:\n",
    "    print(f\"Generating DeepSeek trajectories...\")\n",
    "    SKIP_DEEPSEEK_GEN = False\n",
    "    \n",
    "    ds_trajectories = []\n",
    "    \n",
    "    for q_idx, q in enumerate(tqdm(questions, desc=\"DeepSeek Gen\")):\n",
    "        try:\n",
    "            question_id = f\"ds_q_{q_idx:03d}\"\n",
    "            feedback = random.choice(STRONG_NEGATIVE_FEEDBACK_TEMPLATES)\n",
    "            \n",
    "            # Turn 1\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": q.question},\n",
    "            ]\n",
    "            first_response = generate_deepseek(messages)\n",
    "            first_answer = extract_answer(first_response, q.category)\n",
    "            first_correct = check_answer(first_answer, q)\n",
    "            \n",
    "            # Turn 2\n",
    "            messages.append({\"role\": \"assistant\", \"content\": first_response})\n",
    "            messages.append({\"role\": \"user\", \"content\": feedback})\n",
    "            second_response = generate_deepseek(messages)\n",
    "            second_answer = extract_answer(second_response, q.category)\n",
    "            \n",
    "            answer_changed = (\n",
    "                first_answer and second_answer and\n",
    "                first_answer.lower().strip() != second_answer.lower().strip()\n",
    "            )\n",
    "            label = label_trajectory(first_answer, second_answer, q, \"negative\")\n",
    "            \n",
    "            ds_trajectories.append({\n",
    "                'question_id': question_id,\n",
    "                'question': q.question,\n",
    "                'correct_answer': q.correct_answer,\n",
    "                'category': q.category.value,\n",
    "                'first_response': first_response,\n",
    "                'first_answer': first_answer,\n",
    "                'first_correct': first_correct,\n",
    "                'feedback': feedback,\n",
    "                'second_response': second_response,\n",
    "                'second_answer': second_answer,\n",
    "                'answer_changed': answer_changed,\n",
    "                'label': label.value if hasattr(label, 'value') else label,\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error at q_{q_idx}: {e}\")\n",
    "        \n",
    "        if q_idx % 10 == 0:\n",
    "            clear_gpu_memory()\n",
    "    \n",
    "    deepseek_df = pd.DataFrame(ds_trajectories)\n",
    "    deepseek_df.to_csv(DEEPSEEK_TRAJECTORIES_PATH, index=False)\n",
    "    print(f\"Saved {len(deepseek_df)} DeepSeek trajectories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.5: Extract DeepSeek activations and test cross-model transfer\n",
    "\n",
    "if USING_TRANSFORMERLENS:\n",
    "    # Filter valid trajectories\n",
    "    ds_valid = deepseek_df[\n",
    "        (deepseek_df['first_correct'] == True) & \n",
    "        (deepseek_df['label'].isin(['sycophantic', 'maintained']))\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Extracting DeepSeek activations for {len(ds_valid)} valid samples...\")\n",
    "    \n",
    "    DS_LAYERS = [12, 14, 16, 18, 20]  # Subset for speed\n",
    "    ds_activations = {layer: [] for layer in DS_LAYERS}\n",
    "    ds_labels = []\n",
    "    \n",
    "    for idx, (_, row) in enumerate(tqdm(ds_valid.iterrows(), total=len(ds_valid))):\n",
    "        try:\n",
    "            # Build prompt\n",
    "            prompt = \"\"\n",
    "            for role, content in [\n",
    "                (\"system\", SYSTEM_PROMPT),\n",
    "                (\"user\", row['question']),\n",
    "                (\"assistant\", row['first_response']),\n",
    "                (\"user\", row['feedback']),\n",
    "            ]:\n",
    "                prompt += f\"<|{role}|>\\n{content}\\n\"\n",
    "            prompt += \"<|assistant|>\\n\"\n",
    "            \n",
    "            tokens = deepseek_model.to_tokens(prompt)\n",
    "            _, cache = deepseek_model.run_with_cache(tokens)\n",
    "            \n",
    "            for layer in DS_LAYERS:\n",
    "                act = cache[\"resid_post\", layer][0, -1, :].cpu().to(torch.float32).numpy()\n",
    "                ds_activations[layer].append(act)\n",
    "            \n",
    "            ds_labels.append(1 if row['label'] == 'sycophantic' else 0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "        if idx % 10 == 0:\n",
    "            clear_gpu_memory()\n",
    "    \n",
    "    ds_activations = {k: np.array(v) for k, v in ds_activations.items()}\n",
    "    ds_labels = np.array(ds_labels)\n",
    "    \n",
    "    print(f\"\\nDeepSeek activations: {ds_labels.sum()} sycophantic, {len(ds_labels) - ds_labels.sum()} maintained\")\n",
    "else:\n",
    "    print(\"Skipping DeepSeek activation extraction (nnsight mode)\")\n",
    "    ds_activations = None\n",
    "    ds_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.6: Cross-model probe test\n",
    "\n",
    "if ds_activations is not None and len(np.unique(ds_labels)) > 1:\n",
    "    print(\"CROSS-MODEL TRANSFER TEST\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    cross_model_results = {'llama_on_deepseek': {}, 'deepseek_native': {}, 'cosine_similarity': {}}\n",
    "    \n",
    "    for layer in [16]:  # Test on best Llama layer\n",
    "        if layer not in ds_activations:\n",
    "            continue\n",
    "        \n",
    "        X_ds = ds_activations[layer]\n",
    "        y_ds = ds_labels\n",
    "        \n",
    "        # Test 1: Llama direction on DeepSeek\n",
    "        llama_dir = llama_probe_data['dim_directions'][layer]\n",
    "        llama_dir_norm = llama_dir / np.linalg.norm(llama_dir)\n",
    "        \n",
    "        projections = X_ds @ llama_dir_norm\n",
    "        transfer_auc = roc_auc_score(y_ds, projections)\n",
    "        cross_model_results['llama_on_deepseek'][layer] = transfer_auc\n",
    "        \n",
    "        print(f\"Layer {layer}: Llama direction on DeepSeek -> AUC = {transfer_auc:.3f}\")\n",
    "        if transfer_auc > 0.7:\n",
    "            print(\"  -> TRANSFER SUCCESS!\")\n",
    "        \n",
    "        # Test 2: DeepSeek-native probe\n",
    "        syc_mask = y_ds == 1\n",
    "        ds_dir = X_ds[syc_mask].mean(0) - X_ds[~syc_mask].mean(0)\n",
    "        ds_dir_norm = ds_dir / np.linalg.norm(ds_dir)\n",
    "        \n",
    "        ds_projections = X_ds @ ds_dir_norm\n",
    "        ds_native_auc = roc_auc_score(y_ds, ds_projections)\n",
    "        cross_model_results['deepseek_native'][layer] = ds_native_auc\n",
    "        \n",
    "        print(f\"Layer {layer}: DeepSeek-native DiM -> AUC = {ds_native_auc:.3f}\")\n",
    "        \n",
    "        # Test 3: Direction similarity\n",
    "        cos_sim = np.dot(llama_dir_norm, ds_dir_norm)\n",
    "        cross_model_results['cosine_similarity'][layer] = float(cos_sim)\n",
    "        \n",
    "        print(f\"Layer {layer}: Cosine similarity = {cos_sim:.3f}\")\n",
    "        if abs(cos_sim) > 0.7:\n",
    "            print(\"  -> SHARED MECHANISM!\")\n",
    "    \n",
    "    # Save cross-model results\n",
    "    with open(CROSS_MODEL_RESULTS_PATH, 'w') as f:\n",
    "        json.dump(cross_model_results, f, indent=2)\n",
    "    print(f\"\\nCHECKPOINT SAVED: {CROSS_MODEL_RESULTS_PATH}\")\n",
    "else:\n",
    "    print(\"Skipping cross-model test (insufficient data or nnsight mode)\")\n",
    "    cross_model_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.7: Unload DeepSeek\n",
    "print(\"Unloading DeepSeek model...\")\n",
    "del deepseek_model\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Visualizations\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8.1: Create figures directory\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FIGURES_DIR = OUTPUT_DIR / \"figures\"\n",
    "FIGURES_DIR.mkdir(exist_ok=True)\n",
    "print(f\"Figures will be saved to: {FIGURES_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8.2: Layer sweep plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "layers = llama_probe_data['layers']\n",
    "dim_aucs = [llama_probe_data['dim_aucs'][l] for l in layers]\n",
    "lr_aucs = [llama_probe_data['lr_aucs'][l] for l in layers]\n",
    "\n",
    "ax.plot(layers, dim_aucs, 'o-', label='DiM', markersize=8)\n",
    "ax.plot(layers, lr_aucs, 's-', label='LR', markersize=8)\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Random')\n",
    "ax.axhline(y=0.7, color='green', linestyle=':', alpha=0.5, label='Target (0.7)')\n",
    "\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('ROC-AUC')\n",
    "ax.set_title('Sycophancy Probe Performance by Layer (Llama-3-8B)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'layer_sweep.png', dpi=150)\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'layer_sweep.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8.3: Steering bar chart\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Load steering results\n",
    "with open(STEERING_RESULTS_PATH) as f:\n",
    "    sr = json.load(f)\n",
    "\n",
    "categories = ['Baseline', 'Ablated']\n",
    "rates = [sr['primary']['baseline_rate'] * 100, sr['primary']['ablated_rate'] * 100]\n",
    "colors = ['#ff6b6b', '#4ecdc4']\n",
    "\n",
    "bars = ax.bar(categories, rates, color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax.set_ylabel('Sycophancy Rate (%)')\n",
    "ax.set_title('Effect of Ablating Sycophancy Direction')\n",
    "ax.set_ylim(0, max(rates) * 1.2)\n",
    "\n",
    "# Add value labels\n",
    "for bar, rate in zip(bars, rates):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{rate:.1f}%', \n",
    "            ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add reduction annotation\n",
    "reduction = sr['primary']['reduction'] * 100\n",
    "ax.annotate(f'{reduction:.0f}% reduction', xy=(0.5, max(rates) * 0.7), \n",
    "            fontsize=14, ha='center', color='green', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'steering_comparison.png', dpi=150)\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'steering_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Export & Download\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9.1: Compile summary\n",
    "summary = {\n",
    "    'experiment': 'Sycophancy Detection and Steering',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'llama': {\n",
    "        'model': LLAMA_MODEL,\n",
    "        'n_trajectories': len(llama_df),\n",
    "        'n_valid': len(llama_valid),\n",
    "        'n_sycophantic': int((llama_valid['label'] == 'sycophantic').sum()),\n",
    "        'probe_best_layer': llama_probe_data['best_layer_dim'],\n",
    "        'probe_auc': llama_probe_data['dim_aucs'][llama_probe_data['best_layer_dim']],\n",
    "    },\n",
    "    'steering': {\n",
    "        'layer': sr['steer_layer'],\n",
    "        'baseline_rate': sr['primary']['baseline_rate'],\n",
    "        'ablated_rate': sr['primary']['ablated_rate'],\n",
    "        'reduction': sr['primary']['reduction'],\n",
    "    },\n",
    "}\n",
    "\n",
    "if cross_model_results:\n",
    "    summary['cross_model'] = cross_model_results\n",
    "\n",
    "with open(OUTPUT_DIR / 'summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9.2: List all output files\n",
    "print(\"\\nOUTPUT FILES:\")\n",
    "print(\"=\" * 50)\n",
    "for f in sorted(OUTPUT_DIR.glob('*')):\n",
    "    if f.is_file():\n",
    "        size = f.stat().st_size / 1e6\n",
    "        print(f\"  {f.name}: {size:.1f} MB\")\n",
    "    elif f.is_dir():\n",
    "        print(f\"  {f.name}/\")\n",
    "        for sf in f.glob('*'):\n",
    "            size = sf.stat().st_size / 1e6\n",
    "            print(f\"    {sf.name}: {size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9.3: Create zip for download (Colab only)\n",
    "try:\n",
    "    import shutil\n",
    "    from google.colab import files\n",
    "    \n",
    "    zip_path = '/content/sycophancy_results.zip'\n",
    "    shutil.make_archive('/content/sycophancy_results', 'zip', OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"\\nDownload link:\")\n",
    "    files.download(zip_path)\n",
    "except ImportError:\n",
    "    print(\"Not in Colab - files are in OUTPUT_DIR\")\n",
    "    print(f\"Path: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment Complete!\n",
    "\n",
    "**Key Results:**\n",
    "1. Sycophancy probe AUC at layer 16 (see summary)\n",
    "2. Steering reduces sycophancy rate (see summary)\n",
    "3. Cross-model transfer (if DeepSeek worked)\n",
    "\n",
    "**Files saved to:**\n",
    "- Google Drive (if mounted): `/content/drive/MyDrive/MATS_sycophancy/`\n",
    "- Or local: `experiments/pipeline_run/`\n",
    "\n",
    "**Checkpoint files allow resuming from any section if the notebook crashes.**\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}