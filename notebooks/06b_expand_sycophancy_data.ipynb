{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06b: Expand Sycophancy Dataset\n",
    "\n",
    "**Goal:** Add more sycophantic examples using strong negative feedback\n",
    "\n",
    "**Current state:** 12 sycophantic, 51 maintained (19% rate)\n",
    "**Target:** 30+ sycophantic examples for robust steering test\n",
    "\n",
    "**Strategy:** Use stronger negative feedback on science/geography questions (higher sycophancy rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: Setup (skip if already done)\n",
    "import os\n",
    "\n",
    "if not os.path.exists('/content/IPHR_Direction'):\n",
    "    !git clone https://github.com/nawidayima/IPHR_Direction.git\n",
    "    %cd /content/IPHR_Direction\n",
    "else:\n",
    "    %cd /content/IPHR_Direction\n",
    "    !git pull\n",
    "\n",
    "!pip install torch transformers accelerate pandas tqdm -q\n",
    "!pip install -e . -q\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Restart runtime, then run from Cell 1\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "%cd /content/IPHR_Direction\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "\n",
    "from src.sycophancy import (\n",
    "    QuestionCategory,\n",
    "    SycophancyLabel,\n",
    "    TrajectoryResult,\n",
    "    SYSTEM_PROMPT,\n",
    "    SCIENCE_QUESTIONS,\n",
    "    GEOGRAPHY_QUESTIONS,\n",
    "    STRONG_NEGATIVE_FEEDBACK_TEMPLATES,\n",
    "    extract_answer,\n",
    "    check_answer,\n",
    "    label_trajectory,\n",
    ")\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: HuggingFace Auth\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = None\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if not hf_token and \"HF_TOKEN\" in os.environ:\n",
    "    hf_token = os.environ[\"HF_TOKEN\"]\n",
    "\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"Logged in to HuggingFace\")\n",
    "else:\n",
    "    raise ValueError(\"No HF_TOKEN found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load model\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Config\n",
    "N_RUNS = 3  # Number of times to run each question with different feedback\n",
    "CATEGORIES = [\"science\", \"geography\"]  # Higher sycophancy rate\n",
    "\n",
    "# Get questions\n",
    "questions = SCIENCE_QUESTIONS + GEOGRAPHY_QUESTIONS\n",
    "print(f\"Using {len(questions)} questions from: {CATEGORIES}\")\n",
    "print(f\"Will generate {len(questions) * N_RUNS} new trajectories\")\n",
    "\n",
    "# Show strong feedback templates\n",
    "print(f\"\\nStrong negative feedback templates ({len(STRONG_NEGATIVE_FEEDBACK_TEMPLATES)}):\")\n",
    "for i, fb in enumerate(STRONG_NEGATIVE_FEEDBACK_TEMPLATES[:3]):\n",
    "    print(f\"  {i+1}. {fb[:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Generation function\n",
    "def generate_response(messages, max_new_tokens=100):\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, tokenize=False\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=None,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][prompt_len:], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "# Test\n",
    "test = generate_response([{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, \n",
    "                          {\"role\": \"user\", \"content\": \"What is 2+2?\"}])\n",
    "print(f\"Test: {test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Generate trajectories with STRONG negative feedback\n",
    "all_trajectories = []\n",
    "\n",
    "for run_idx in range(N_RUNS):\n",
    "    print(f\"\\n=== Run {run_idx + 1}/{N_RUNS} ===\")\n",
    "    \n",
    "    for q_idx, q in enumerate(tqdm(questions, desc=f\"Run {run_idx+1}\")):\n",
    "        question_id = f\"q_strong_{run_idx}_{q_idx:03d}\"\n",
    "        feedback = random.choice(STRONG_NEGATIVE_FEEDBACK_TEMPLATES)\n",
    "        \n",
    "        try:\n",
    "            # Turn 1: Ask question\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": q.question},\n",
    "            ]\n",
    "            first_response = generate_response(messages)\n",
    "            first_answer = extract_answer(first_response, q.category)\n",
    "            first_correct = check_answer(first_answer, q)\n",
    "            \n",
    "            # Turn 2: Strong negative feedback\n",
    "            messages.append({\"role\": \"assistant\", \"content\": first_response})\n",
    "            messages.append({\"role\": \"user\", \"content\": feedback})\n",
    "            second_response = generate_response(messages)\n",
    "            second_answer = extract_answer(second_response, q.category)\n",
    "            \n",
    "            # Check if answer changed\n",
    "            if first_answer and second_answer:\n",
    "                answer_changed = first_answer.lower().strip() != second_answer.lower().strip()\n",
    "            else:\n",
    "                answer_changed = False\n",
    "            \n",
    "            label = label_trajectory(first_answer, second_answer, q, \"negative\")\n",
    "            \n",
    "            traj = TrajectoryResult(\n",
    "                question_id=question_id,\n",
    "                question=q.question,\n",
    "                correct_answer=q.correct_answer,\n",
    "                category=q.category.value,\n",
    "                first_response=first_response,\n",
    "                first_answer=first_answer,\n",
    "                first_correct=first_correct,\n",
    "                feedback_type=\"negative_strong\",\n",
    "                feedback=feedback,\n",
    "                second_response=second_response,\n",
    "                second_answer=second_answer,\n",
    "                answer_changed=answer_changed,\n",
    "                label=label,\n",
    "            )\n",
    "            all_trajectories.append(traj)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError at {question_id}: {e}\")\n",
    "        \n",
    "        if q_idx % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nGenerated {len(all_trajectories)} new trajectories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Analyze new trajectories\n",
    "df_new = pd.DataFrame([t.to_dict() for t in all_trajectories])\n",
    "\n",
    "valid_new = df_new[df_new['first_correct'] == True]\n",
    "n_syc_new = (valid_new['label'] == 'sycophantic').sum()\n",
    "n_maintained_new = (valid_new['label'] == 'maintained').sum()\n",
    "\n",
    "print(\"New Data (strong feedback):\")\n",
    "print(f\"  Total: {len(df_new)}\")\n",
    "print(f\"  Valid: {len(valid_new)}\")\n",
    "print(f\"  Sycophantic: {n_syc_new} ({n_syc_new/len(valid_new)*100:.1f}%)\")\n",
    "print(f\"  Maintained: {n_maintained_new}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Load and merge with existing data\n",
    "import glob\n",
    "\n",
    "# Find existing sycophancy.csv\n",
    "existing_files = sorted(glob.glob(\"experiments/run_*_sycophancy/trajectories/sycophancy.csv\"))\n",
    "if existing_files:\n",
    "    existing_path = existing_files[-1]\n",
    "    df_existing = pd.read_csv(existing_path)\n",
    "    print(f\"Loaded existing data from: {existing_path}\")\n",
    "    print(f\"  Existing rows: {len(df_existing)}\")\n",
    "    \n",
    "    # Merge\n",
    "    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "    print(f\"  Combined rows: {len(df_combined)}\")\n",
    "else:\n",
    "    print(\"No existing data found, using new data only\")\n",
    "    df_combined = df_new\n",
    "    existing_path = \"experiments/run_new_sycophancy/trajectories/sycophancy.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Final statistics\n",
    "valid_all = df_combined[(df_combined['first_correct'] == True) & \n",
    "                        (df_combined['feedback_type'].str.contains('negative'))]\n",
    "\n",
    "n_syc_total = (valid_all['label'] == 'sycophantic').sum()\n",
    "n_maintained_total = (valid_all['label'] == 'maintained').sum()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COMBINED DATASET STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total trajectories: {len(df_combined)}\")\n",
    "print(f\"Valid negative feedback: {len(valid_all)}\")\n",
    "print(f\"Sycophantic: {n_syc_total} ({n_syc_total/len(valid_all)*100:.1f}%)\")\n",
    "print(f\"Maintained: {n_maintained_total} ({n_maintained_total/len(valid_all)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "if n_syc_total >= 30:\n",
    "    print(\"SUCCESS: Target of 30+ sycophantic examples reached!\")\n",
    "else:\n",
    "    print(f\"Need {30 - n_syc_total} more sycophantic examples. Consider running more iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Save combined dataset\n",
    "# Save to same directory as original\n",
    "output_dir = Path(existing_path).parent\n",
    "output_path = output_dir / \"sycophancy_expanded.csv\"\n",
    "\n",
    "df_combined.to_csv(output_path, index=False)\n",
    "print(f\"Saved to: {output_path}\")\n",
    "\n",
    "# Also save as the main file for notebook 07\n",
    "main_path = output_dir / \"sycophancy.csv\"\n",
    "df_combined.to_csv(main_path, index=False)\n",
    "print(f\"Also saved to: {main_path} (for notebook 07)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now run:\n",
    "1. **Notebook 07** - Re-extract activations with expanded dataset\n",
    "2. **Notebook 08** - Retrain probes\n",
    "3. **Notebook 09** - Steering experiment\n",
    "4. **Notebook 10** - DeepSeek cross-model test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
